name: Data Pipeline

on:
  schedule:
    # Backup trigger every 6h in case Airflow is the primary scheduler
    # UTC: 00:00, 06:00, 12:00, 18:00
    - cron: "0 */6 * * *"
  workflow_dispatch:
    inputs:
      dag_id:
        description: "Airflow DAG to trigger"
        required: true
        default: "ingest_flights"
        type: choice
        options:
          - ingest_flights
          - ingest_hostels
          - run_dbt_transforms
          - score_and_alert
          - cleanup_old_prices

jobs:
  trigger-airflow:
    name: Trigger Airflow DAG
    runs-on: ubuntu-latest

    steps:
      - name: Trigger DAG via Airflow REST API
        # Pass user-controlled workflow_dispatch input via env var (not directly
        # into the shell script) to avoid command injection.
        env:
          DAG_ID: ${{ github.event.inputs.dag_id || 'ingest_flights' }}
          AIRFLOW_BASE_URL: ${{ secrets.AIRFLOW_BASE_URL }}
          AIRFLOW_CREDENTIALS: ${{ secrets.AIRFLOW_USER }}:${{ secrets.AIRFLOW_PASS }}
        run: |
          echo "Triggering DAG: $DAG_ID"

          HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" \
            -X POST \
            "$AIRFLOW_BASE_URL/api/v1/dags/$DAG_ID/dagRuns" \
            -H "Content-Type: application/json" \
            -u "$AIRFLOW_CREDENTIALS" \
            -d '{"conf": {"triggered_by": "github_actions"}}')

          echo "HTTP status: $HTTP_STATUS"
          if [ "$HTTP_STATUS" != "200" ] && [ "$HTTP_STATUS" != "201" ]; then
            echo "Failed to trigger DAG (HTTP $HTTP_STATUS)"
            exit 1
          fi
          echo "DAG triggered successfully"
